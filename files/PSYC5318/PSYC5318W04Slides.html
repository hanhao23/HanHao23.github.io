<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Confirmatory Factor Analysis: Concepts and Executive Workflow</title>
    <meta charset="utf-8" />
    <meta name="author" content="Han Hao" />
    <script src="libs/header-attrs-2.29/header-attrs.js"></script>
    <link rel="stylesheet" href="xaringan-themer.css" type="text/css" />
    <link rel="stylesheet" href="hh.css" type="text/css" />
    <link rel="stylesheet" href="scrollable.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">




class: inverse middle

# Confirmatory Factor Analysis

### PSYC 5318 Week 04
#### Han Hao 2026

---
class: inverse
layout: true

## Agenda (For these two weeks)

---

1. What CFA is and what it “confirms”
2. CFA vs EFA (same backbone, different stance)
3. Statistical foundations (measurement model + model-implied covariance)
4. Identification and scaling (why df matters)
5. Model fit: χ² + common fit indices (equations + heuristics)
6. Multiple plausible CFA specifications (continue)
   - 1-factor model
   - 3-factor correlated model (Fluid, Verbal, Spatial)

---

6. Multiple plausible CFA specifications (continue)
   - Higher-order model
   - Bifactor model
7. Diagnostics and ethical respecification
   - Modification indices
   - Correlated residuals
8. Practical workflow + reporting checklist
9. Lab 2 Demo: CFA Practice in R

???

Goal: you should be able to translate a theoretical statement into a CFA model, anticipate what constraints it implies, and interpret global + local evidence without “fit-chasing.”

---
layout: false

## What CFA is in one sentence

&gt; **CFA evaluates whether a prespecified latent structure can reproduce the observed covariance (or correlation) matrix well enough.**

- **Prespecified** = theory/insights-driven constraints

- **Reproduce covariance** = match inter-variable relations, not raw observations
  - You may still "reproduce" the raw observations by knowing the Ms and SDs
  
- **“Well enough”** = multiple fit summaries + parameter plausibility + model diagnostics

???

“Confirmatory” means “constraint-driven and testable,” not “proven true.”

---

## CFA to “confirm”

Common purposes in CFA are confirming **the detailed measurement structure** of corresponding latent constructs:

- Which indicators define each factor (**mapping of factor structure**)

- Which cross-loadings can be **forced** as 0 (simple structure of factors)

- Whether factors are correlated (and how strongly)

- Whether residuals are uncorrelated (often a starting assumption in estimation)

- .pt[Equal constraints (multi-group or longitudinal invariance; more in W07)]

???

In CFA, you declare assumptions up front so the data can disagree in interpretable ways.

---

## CFA vs EFA

Both use the same measurement foundation:

`$$\begin{align}
x_1 = \mu_1 + {\lambda_1}_1 f_1 + {\lambda_1}_2 f_2 + \cdots + \epsilon_1 \\
x_2 = \mu_2 + {\lambda_2}_1 f_1 + {\lambda_2}_2 f_2 + \cdots + \epsilon_2 \\
x_3 = \mu_3 + {\lambda_3}_1 f_1 + {\lambda_3}_2 f_2 + \cdots + \epsilon_3 \\
\end{align}$$`

Or simply,

\\[
x = \mu + \Lambda f + \epsilon
\\]

But they ask different questions: **what is fixed vs free** (constraints in pattern matrix).

???

EFA asks “what low-dimensional structure is suggested by data?” CFA asks “does this particular theory-consistent structure hold up?”

---

.pull-left[
## EFA as a data-driven tool

- Loadings are freely estimated, then rotated for interpretability

- Structure emerges from data + rotation decisions

- Best for early scale development, item pools, new task batteries
]

.pull-right[
![:scale 90%](image/EFAVSCFA.jpeg)
]

--

???

EFA is often where measurement begins.

---

.pull-left[
## CFA as a theory-driven tool

- Many loadings fixed to 0 by design (simple structure)

- Model explicitly declares factor correlations and residual structures
  - test theory-driven constraints
  - compare competing models
]

.pull-right[
![:scale 90%](image/EFAVSCFA.jpeg)
]

???

CFA is most valuable when constraints are meaningful scientific claims.

---

## Foundations: covariance estimation

The fundamental equation stays the same:

\\[
\mathbf{\Sigma} = \mathbf{\Lambda}\mathbf{\Phi}\mathbf{\Lambda}^\top + \mathbf{\Psi}
\\]

- \\(\mathbf{\Lambda}\\): factor loadings
- \\(\mathbf{\Phi}\\): factor covariance matrix
- \\(\mathbf{\Psi}\\): residual covariance matrix

CFA estimates parameters to reproduce a **model-implied convariance matrix** `\(\Sigma\)` that approximates the **sample (observed) covariance matrix** `\(S\)`.

???



---

## For one CFA model

- **Global fit**: does `\(\Sigma\)` approximate `\(S\)` overall?
  - χ², CFI, TLI, RMSEA, SRMR
  - All these “fit indices” are different ways to summarize how close `\(\Sigma\)`  is to `\(S\)`
  
- **Local fit**: where and why is the "misfits"?
  - Residuals (`\(S - \Sigma\)`): unexplained residual covariances
  - Weak loadings, Heywood cases: manifest variable issues? Mis-specifications?
  - Factor correlations and other issues
  - Modification indices (MI) for dignostics suggestions


???

Never stop at global fit. Local diagnostics tell you what is wrong, and whether “fixes” make theoretical sense.

---

## For competing CFA models

### **Model comparison = Theory comparison**

In CFA, each CFA model corresponds to a different measurement theory claims

  - the operationalizational definition of the (structure) of the latent construct(s)
  
  - “Best fit” is not automatically “best theory”
  
  - Flexible models can win on fit but lose on interpretability, stability, or generalizability

---

### Suggested practice

1. **Specify a small set of plausible models a priori** (or label post-hoc as exploratory)
2. Fit each model with the **same estimator / same data handling**
3. Compare using *converging evidence*:
  - **Model comparison tests**
  - **global fit** (some indices consider accuracy-parsimony trade-off)
  - **parameter plausibility** (loadings, residual variances, factor correlations)
  - **local fit for exploration** (residuals; MI only as diagnostics)
4. Prefer the model that is adequately **fitting**, theoretically **meaningful**, and **not overly specific** to the at-hand data
5. If modifications are made, keep them **minimal, justified**
6. The preferred model may need further validations


---

## Running example: EFASimData 

The same observed variables and subjects as in the EFA lecture, but now the models are declared a priori

- Fluid: F1 F2 F3
- Verbal: V1 V2 V3
- Spatial: S1 S2 S3

In this course, we will use the "**lavaan**" package ("**la**tent **va**riable **an**alysis"):


``` r
# install.packages("lavaan")
library(lavaan)
```

---

.pull-left[
### Correlations among the variables

What you hope to see:

- higher within-domain correlations (F–F, V–V, S–S)
- moderate across-domain correlations (positive manifold)
]

.pull-right[

![](PSYC5318W04Slides_files/figure-html/unnamed-chunk-8-1.svg)&lt;!-- --&gt;
]

???

This is not “doing EFA again.” It’s a sanity check that your measurement story is at least plausible.


---

### Multiple plausible models (competing theories)
.pull-left[
Same indicators can support different models depending on the theory:

1. 1-factor model (g)
2. 3 correlated factors (Fluid, Verbal, Spatial)

#### There are other ways of model specifications that could account for different theoretical indications, but we will focus on these two this week.
]

.pull-right[
![:scale 65%](image/1FVS3F.jpeg)
]

???

This section is about scientific flexibility done responsibly: different models correspond to different theoretical claims.

---

### Model 1: One-factor (general ability)

An unidimensional **reflective** theory of intelligence: All 9 indicators are imperfect reflections of the latent construct **g**. Any sub-domain clustering is negligible.

&gt; What should be the directions of the arrows in a diagram?

Grammar for model specification in lavaan (.gt[the simple version]):


``` r
m1 &lt;- '
g =~ F1 + F2 + F3 + V1 + V2 + V3 + S1 + S2 + S3
'
```

The operator "**=~**" connects the named **reflective** factor (left) and the corresponding manifest variables (right)

???

If this fits well and loadings are strong, you might argue a strong general factor story (at least at this measurement level).

---
### Model 1: fitting results

Fitting the specified model (.gt[*the simple version*]) with the "cfa()" function:
&gt; This surpasses the basic lavaan() function because it sets a few default arguments that automatically specify residual variances, factor variances and factor covariances for CFA models, so we can use a simple version of model specification (our "m1") instead of a full set of specifications.


``` r
# Estimate the model (Maximum likelihood and correlated factors)
m1_result &lt;- cfa(m1, data = CFAvars,
                 estimator = "ML", orthogonal = F)
# Summarize the estimated results
# Provide fit indices and standardized loadings
summary(m1_result, standardized = T, fit.measures = T)
```

---

## Scaling

Latent variables have no natural scale, so we need to arbitrarily set some scales for the estimated parameters (loadings and variances).

Two common scale-setting choices:

1. **Marker loading**: fix one loading per factor to 1 (free estimation of factor variances)

2. **Unit variance**: fix each factor variance to 1 (free estimation of all loadings, recommended and default in my illustrations)

???

Scaling is not just a technicality. It affects the meaning of unstandardized parameters.

---

## Identification

#### **Identification is about ensuring you have enough information to find an unique answer.**

&gt; A model must be **identified** to estimate unique parameters.

In general, we need the number of estimated parameters smaller than the number of observations:

&gt; If we observed 3 "things", and we used 3 "things" to describe them, is this really a **useful** model?

Model identification can be quantified by the **degrees of freedom** of the model, which quantifies the available information for estimation in the specified model.

---

## Degrees of freedom

Unique elements in observed covariances matrix (`\(S\)`) for `\(p\)` observed variables is our total number of "observations" from the data:

\\[
\frac{p(p+1)}{2}
\\]

####  Model degrees of freedom df = # of unique elements in S - # of estimated parameters

- df &gt; 0 overidentified: **testable model**
- df = 0 just-identified: saturated model
- df &lt; 0 underidentified: cannot estimate

???

CFA becomes interesting when df &gt; 0 because constraints create a testable prediction.

---

.pull-left[
### df in Model 1: 9 indicators, one-factor
p = 9, so 9*(9 + 1)/2 = 45 unique elements in covariance matrix.

The **correlation** matrix here has 36 unique correlations ("standardized covariances").

**Where are the other 9 unique observations?**

]

.pull-right[

![](PSYC5318W04Slides_files/figure-html/unnamed-chunk-11-1.svg)&lt;!-- --&gt;
]

???

Your constraints mostly come from fixing cross-loadings to 0.

---

.pull-left[
### df in Model 1: 9 indicators, one-factor
p = 9, so 9*(9 + 1)/2 = 45 unique elements in covariance matrix.

Free parameters to estimate:
- 9 loadings (for unit-var scaling)
- 9 residual variances
- 0 factor variance (unit-var scaling fixes them all to 1)
- 0 factor/residual correlation

]

.pull-right[

![:scale 95%](image/1F.jpeg)

&gt; Total estimated parameters =   
9 + 9 + 0 + 0 = 18  
So, df = 45 − 18 = 27
]

---
### Model 1: fitting results


``` r
m1_result &lt;- cfa(m1, data = CFAvars, std.lv = T,
                 estimator = "ML", orthogonal = F)
summary(m1_result, standardized = T, fit.measures = T)
```

```
lavaan 0.6-19 ended normally after 21 iterations

  Estimator                                         ML
  Optimization method                           NLMINB
  Number of model parameters                        18

  Number of observations                           250

Model Test User Model:
                                                      
  Test statistic                               215.437
  Degrees of freedom                                27
  P-value (Chi-square)                           0.000

Model Test Baseline Model:

  Test statistic                               875.062
  Degrees of freedom                                36
  P-value                                        0.000

User Model versus Baseline Model:

  Comparative Fit Index (CFI)                    0.775
  Tucker-Lewis Index (TLI)                       0.701

Loglikelihood and Information Criteria:

  Loglikelihood user model (H0)              -9617.626
  Loglikelihood unrestricted model (H1)      -9509.907
                                                      
  Akaike (AIC)                               19271.252
  Bayesian (BIC)                             19334.639
  Sample-size adjusted Bayesian (SABIC)      19277.577

Root Mean Square Error of Approximation:

  RMSEA                                          0.167
  90 Percent confidence interval - lower         0.147
  90 Percent confidence interval - upper         0.188
  P-value H_0: RMSEA &lt;= 0.050                    0.000
  P-value H_0: RMSEA &gt;= 0.080                    1.000

Standardized Root Mean Square Residual:

  SRMR                                           0.108

Parameter Estimates:

  Standard errors                             Standard
  Information                                 Expected
  Information saturated (h1) model          Structured

Latent Variables:
                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all
  g =~                                                                  
    F1               15.448    1.001   15.439    0.000   15.448    0.832
    F2               15.641    1.101   14.209    0.000   15.641    0.787
    F3               15.797    1.051   15.031    0.000   15.797    0.818
    V1               10.570    1.352    7.821    0.000   10.570    0.491
    V2                9.873    1.341    7.361    0.000    9.873    0.466
    V3                9.569    1.254    7.629    0.000    9.569    0.481
    S1               10.560    1.280    8.247    0.000   10.560    0.514
    S2                9.450    1.320    7.158    0.000    9.450    0.454
    S3                8.850    1.244    7.116    0.000    8.850    0.452

Variances:
                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all
   .F1              105.726   13.630    7.757    0.000  105.726    0.307
   .F2              150.825   17.279    8.729    0.000  150.825    0.381
   .F3              123.828   15.251    8.119    0.000  123.828    0.332
   .V1              351.577   32.815   10.714    0.000  351.577    0.759
   .V2              352.110   32.681   10.774    0.000  352.110    0.783
   .V3              304.930   28.393   10.740    0.000  304.930    0.769
   .S1              310.254   29.126   10.652    0.000  310.254    0.736
   .S2              343.603   31.817   10.799    0.000  343.603    0.794
   .S3              305.396   28.266   10.804    0.000  305.396    0.796
    g                 1.000                               1.000    1.000
```

---

## Estimation Method (conceptual)

Goal: find \\(\theta\\) that makes \\(\Sigma(\theta)\\) close to \\(S\\).

Common estimators we will encounter in lavaan:

- **ML** (maximum likelihood): works well for normal numeric data
- Robust ML variants
  - Corrects statistics based on the level of non-normality
  - **MLM** (Satorra-Bentler correction)
  - **MLR** (Huber-White correction)
- **WLSMV** (Weighted Least Squares Mean and Variance adjusted)
  - Often preferred for ordinal data

???

Estimator choice affects χ², SEs, and some fit indices. For ordinal items, treating them as continuous can distort inferences.

---

### Maximum likelihood (ML): the basic idea

&gt; In plain language,ML picks the model parameters that make the data you observed **most plausible** under your model.  

In SEM/CFA, **ML chooses parameters (`\(\theta\)`)** to make `\(\Sigma\)` as close as possible to `\(S\)` in a likelihood sense:

\\[
\hat{\theta}_{ML} = \arg \max L(\theta | S)
\\]

&gt; .rt[The estimation of the parameters based on ML] (`\(\hat{\theta}_{ML}\)`) finds the value of parameters `\(\theta\)` that maximize the .bt[likelihood of observing S given those parameters] (`\(L(\theta | S)\)`)


---

### ML in SEM as a "minimal distance" solution

For multivariate normal data, maximizing likelihood is (almost) equivalent to minimizing a mismatch function between `\(S\)` and `\(\Sigma\)`.

A common form of the ML fitting function is:

\\[
F_{ML}(\theta) = \log |\Sigma(\theta)| + \mathrm{tr}\left(S\Sigma(\theta)^{-1}\right) - \log|S| - p
\\]

You don’t need to memorize the formula. The key interpretation is that it quantifies **how different** `\(S\)` is from \\(\Sigma(\theta)\\) for the k manifest variables
- ML finds `\(\theta\)` that **minimizes** this mismatch (and **maximizes** `\(L(\theta | S)\)`)

---

## Model fit: exact test and common fit indices

&gt; We use multiple statistics and fit indices to evaluate the global fit of a model, because each index has limitations.

For a recommended set of indices to evaluate, we will cover:

- χ² (exact fit test)
- RMSEA (approximation error per df)
- CFI / TLI (incremental fit vs null)
- SRMR (residual-based)
- AIC / BIC (useful for non-nested comparisons)

---

### χ² (exact fit test statistics)

.pull-left[
`$$\begin{align}
H_0: \Sigma(\theta) = S \\
H_1: \Sigma(\theta) \neq S \\
\end{align}$$`

ML-based test statistic is approximately in a `\(\chi^2\)` distribution on the df of the model:

$$
\chi^2 \approx (N-1)F_{ML}
$$

`\(F_{ML}\)` is the maximum likelihood fit function capturing the difference between S and `\(\Sigma\)`.
]

.pull-right[

So:
better fit → smaller `\(F_{ML}\)` → smaller χ²  
worse fit → larger `\(F_{ML}\)` → larger χ²  
For large N?
-  **χ² is best not to be treated as the single criterion of fitness**

In practice we sometimes also report `\(\chi^2/df\)` .rt[(≤ 3 "good")]
]

???

It is normal for χ² to reject in large samples even when the model is “good enough” for measurement purposes.

---

### CFI: Comparative Fit Index

$$
CFI = 1 - \frac{\max(\chi^2 - df, 0)}{\chi^2_0 - df_0}
$$

Heuristics (context-dependent):

- ≥ .90 "acceptable"; ≥ .95 "great"

???

CFI is relative to an “independence model.” If your indicators are weakly correlated, CFI can be hard to impress.

---

### TLI: Tucker-Lewis Index

Common form:

\\[
TLI = \frac{\chi^2_0/df_0 - \chi^2/df}
{\chi^2_0/df_0 - 1}
\\]

Heuristics:
- similar to CFI: ≥ .90 "acceptable"; ≥ .95 "great"

???

TLI penalizes complexity more strongly and can exceed 1 or go negative. Treat as a descriptive index.

---

### RMSEA
### Root Mean Square Error of Approximation

\\[
\text{RMSEA} = \sqrt{\max\left(\frac{\chi^2 - df}{df(N-1)},\,0\right)}
\\]

Heuristics:
- Report RMSEA with the corresponding **confidence interval**
- ≤ .06 "good" (Upper bound &lt; .08 or .10)

???

RMSEA is sensitive to df and can behave oddly in very low-df models.

---

### SRMR
#### Standardized Root Mean Square Residual


\\[
SRMR = \sqrt{2\sum ({r_i}_j - \hat {r_i}_j)^2/k(k+1)}
\\]

SRMR summarizes the average standardized differences between observed and implied correlations/covariances.

Heuristic:

- ≤ .08 "good"

???

SRMR is intuitive: it is directly tied to residuals, so it connects naturally to local fit inspection.

---

### AIC / BIC (information criteria)

$$
AIC = -2ln(L) + 2k;\
BIC = -2ln(L) + ln(n)k
$$

With k = number of estimated parameters and n = sampe size  

When models are **not nested**, AIC/BIC can be useful, and BIC penalizes complexity more strongly than AIC.

Lower is better, but:

- only compare models fit to the **same data** with the **same estimator**
- use theory to decide what models are meaningful competitors

???

AIC/BIC are not “fit indices” in the same sense as CFI/RMSEA, but they are common in applied CFA papers.

---

.pull-left[

### A simple “fit checklist”

Fit Indices to Report:
- `\(\chi^2\)`(df), p; `\(\chi^2/df\)`
- CFI, TLI
- RMSEA (CI)
- SRMR
- AIC and BIC for comparisons

- **Fit indices only help you make a decision; they can’t make it for you!**
]

.pull-right[

![:scale 90%](image/fits.jpg)

]


---

### Model 2: Three correlated factors

Hypothesis and Implication?
- Each indicator loads on its domain factor
- Factors correlate (we use "**~~**" for correlations).


``` r
m3c &lt;- '
Fluid  =~ F1 + F2 + F3
Verbal =~ V1 + V2 + V3
Spatial =~ S1 + S2 + S3
# Optional if using arguments in the cfa() function
Fluid ~~ Verbal
Fluid ~~ Spatial
Verbal ~~ Spatial
'
```

???

This is often the default starting point when you have domain theory plus a reason to expect correlations.

---

### df calculation

p = 9, so 9(10)/2 = 45 unique observed moments in covariance matrix.

Free parameters:

- 9 loadings (after scaling constraints are handled)
- 9 residual variances
- 3 factor correlations

Total = 21, so df = 45 − 21 = 24

???

Your constraints mostly come from fixing cross-loadings to 0.

---

### Model 2: fitting results


``` r
m3c_result &lt;- cfa(m3c, data = CFAvars, orthogonal = F, std.lv = T) # Try "T"
summary(m3c_result, standardized = T, fit.measures = T)
```

```
lavaan 0.6-19 ended normally after 22 iterations

  Estimator                                         ML
  Optimization method                           NLMINB
  Number of model parameters                        21

  Number of observations                           250

Model Test User Model:
                                                      
  Test statistic                                31.470
  Degrees of freedom                                24
  P-value (Chi-square)                           0.141

Model Test Baseline Model:

  Test statistic                               875.062
  Degrees of freedom                                36
  P-value                                        0.000

User Model versus Baseline Model:

  Comparative Fit Index (CFI)                    0.991
  Tucker-Lewis Index (TLI)                       0.987

Loglikelihood and Information Criteria:

  Loglikelihood user model (H0)              -9525.642
  Loglikelihood unrestricted model (H1)      -9509.907
                                                      
  Akaike (AIC)                               19093.285
  Bayesian (BIC)                             19167.235
  Sample-size adjusted Bayesian (SABIC)      19100.664

Root Mean Square Error of Approximation:

  RMSEA                                          0.035
  90 Percent confidence interval - lower         0.000
  90 Percent confidence interval - upper         0.066
  P-value H_0: RMSEA &lt;= 0.050                    0.754
  P-value H_0: RMSEA &gt;= 0.080                    0.006

Standardized Root Mean Square Residual:

  SRMR                                           0.032

Parameter Estimates:

  Standard errors                             Standard
  Information                                 Expected
  Information saturated (h1) model          Structured

Latent Variables:
                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all
  Fluid =~                                                              
    F1               15.427    1.007   15.317    0.000   15.427    0.831
    F2               16.122    1.091   14.778    0.000   16.122    0.811
    F3               16.328    1.041   15.681    0.000   16.328    0.845
  Verbal =~                                                             
    V1               16.245    1.358   11.962    0.000   16.245    0.755
    V2               14.425    1.355   10.647    0.000   14.425    0.680
    V3               13.868    1.268   10.934    0.000   13.868    0.696
  Spatial =~                                                            
    S1               15.047    1.317   11.428    0.000   15.047    0.733
    S2               14.175    1.344   10.543    0.000   14.175    0.681
    S3               13.327    1.266   10.527    0.000   13.327    0.680

Covariances:
                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all
  Fluid ~~                                                              
    Verbal            0.597    0.057   10.558    0.000    0.597    0.597
    Spatial           0.610    0.057   10.784    0.000    0.610    0.610
  Verbal ~~                                                             
    Spatial           0.356    0.077    4.628    0.000    0.356    0.356

Variances:
                   Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all
   .F1              106.356   14.074    7.557    0.000  106.356    0.309
   .F2              135.543   16.739    8.097    0.000  135.543    0.343
   .F3              106.775   14.947    7.144    0.000  106.775    0.286
   .V1              199.392   29.425    6.776    0.000  199.392    0.430
   .V2              241.515   29.053    8.313    0.000  241.515    0.537
   .V3              204.157   25.435    8.027    0.000  204.157    0.515
   .S1              195.342   27.527    7.096    0.000  195.342    0.463
   .S2              231.984   28.540    8.128    0.000  231.984    0.536
   .S3              206.119   25.307    8.145    0.000  206.119    0.537
    Fluid             1.000                               1.000    1.000
    Verbal            1.000                               1.000    1.000
    Spatial           1.000                               1.000    1.000
```

---

## Model comparison


``` r
anova(m1_result, m3c_result)
```

Identify competing models **before** seeing results (or label post-hoc as exploratory)  
Prefer comparisons that answer a clear theoretical question: Is g alone enough (1-factor) vs domains (3-factor)?

Then interpret:
- test-statistics comparison + fit indices + parameter plausibility (+ generalizability)
- not just “lowest chi-square and best CFI wins”

---
## Model comparison


``` r
anova(m1_result, m3c_result)
```

```

Chi-Squared Difference Test

           Df   AIC   BIC  Chisq Chisq diff   RMSEA Df diff Pr(&gt;Chisq)    
m3c_result 24 19093 19167  31.47                                          
m1_result  27 19271 19335 215.44     183.97 0.49121       3  &lt; 2.2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
```

---

### Diagnostics: modification indices and correlated residuals

These are powerful tools that can also enable questionable practices if misused.

We’ll cover:
- what MI are
- when correlated residuals are defensible
- an ethical respecification workflow

???

Key theme: transparency + theoretical justification + limits + validation.

---

### Modification indices (MI): what they are


``` r
modindices(m1_result, sort = T)
```

MI for a fixed parameter ≈ expected drop in χ² if that parameter were freed.  

It is only a **data-driven** diagnostic.  

Typical uses:
- suggest a cross-loading
- suggest correlating residuals
- suggest correlating factors

???

MI answer: “If you freed this, fit would improve.” It does not answer: “Should you free this?”

---

### Modification indices (MI): what they are


``` r
modindices(m1_result, sort = T)
```

```
##    lhs op rhs     mi     epc sepc.lv sepc.all sepc.nox
## 42  V1 ~~  V3 39.782 136.546 136.546    0.417    0.417
## 41  V1 ~~  V2 38.402 143.895 143.895    0.409    0.409
## 54  S1 ~~  S3 32.324 115.685 115.685    0.376    0.376
## 55  S2 ~~  S3 31.298 118.879 118.879    0.367    0.367
## 53  S1 ~~  S2 31.214 120.617 120.617    0.369    0.369
## 46  V2 ~~  V3 26.257 110.659 110.659    0.338    0.338
## 28  F2 ~~  F3 20.905  63.913  63.913    0.468    0.468
## 35  F3 ~~  V1  8.410 -46.947 -46.947   -0.225   -0.225
## 44  V1 ~~  S2  7.002 -60.617 -60.617   -0.174   -0.174
## 33  F2 ~~  S2  6.364 -42.401 -42.401   -0.186   -0.186
## 45  V1 ~~  S3  6.228 -53.881 -53.881   -0.164   -0.164
## 32  F2 ~~  S1  5.537 -38.131 -38.131   -0.176   -0.176
## 36  F3 ~~  V2  5.026 -36.066 -36.066   -0.173   -0.173
## 37  F3 ~~  V3  4.831 -33.037 -33.037   -0.170   -0.170
## 31  F2 ~~  V3  3.105 -28.064 -28.064   -0.131   -0.131
## 34  F2 ~~  S3  2.792 -26.462 -26.462   -0.123   -0.123
## 23  F1 ~~  V2  2.414 -23.699 -23.699   -0.123   -0.123
## 50  V3 ~~  S1  2.209 -30.323 -30.323   -0.099   -0.099
## 48  V2 ~~  S2  2.092 -33.051 -33.051   -0.095   -0.095
## 51  V3 ~~  S2  1.720 -27.942 -27.942   -0.086   -0.086
## 40  F3 ~~  S3  1.642 -19.135 -19.135   -0.098   -0.098
## 21  F1 ~~  F3  1.306  15.297  15.297    0.134    0.134
## 25  F1 ~~  S1  1.248 -16.241 -16.241   -0.090   -0.090
## 49  V2 ~~  S3  0.970 -21.208 -21.208   -0.065   -0.065
## 29  F2 ~~  V1  0.834 -15.659 -15.659   -0.068   -0.068
## 27  F1 ~~  S3  0.820 -12.812 -12.812   -0.071   -0.071
## 38  F3 ~~  S1  0.773 -13.465 -13.465   -0.069   -0.069
## 39  F3 ~~  S2  0.451 -10.648 -10.648   -0.052   -0.052
## 20  F1 ~~  F2  0.338   7.863   7.863    0.062    0.062
## 30  F2 ~~  V2  0.315  -9.567  -9.567   -0.042   -0.042
## 52  V3 ~~  S3  0.230   9.634   9.634    0.032    0.032
## 47  V2 ~~  S1  0.173   9.110   9.110    0.028    0.028
## 22  F1 ~~  V1  0.158   6.099   6.099    0.032    0.032
## 26  F1 ~~  S2  0.143   5.685   5.685    0.030    0.030
## 24  F1 ~~  V3  0.050  -3.185  -3.185   -0.018   -0.018
## 43  V1 ~~  S1  0.008  -2.018  -2.018   -0.006   -0.006
```

---

## MI: how to use them ethically

A practical ethical rule-set:

1. **Start from a defensible theory model** (don’t start from MI)
2. Use MI to generate a *short list* of candidates
3. Only free parameters with a clear substantive rationale
4. Make small, interpretable changes (avoid chains of tweaks)
5. Treat post-hoc changes as exploratory
6. Validate: replicate or cross-validate (new sample or split sample)
7. Report everything you did and why

???

MI-driven tweaking without theory is basically turning CFA back into a disguised EFA.

---

### Correlated residuals: what they mean

&gt; **After accounting for the factor(s), these two indicators still share covariance.**

Possible substantive reasons:
- shared method effects (same wording, same format, same rater)
- similar content (near-parallel items)
- shared administrative artifact

Possible problematic reasons:
- compensating for misspecified factor structure
- fit-chasing based on only MIs

???

Correlated residuals are not automatically wrong, but they are a strong claim about leftover shared variance.
---
## Visualizations

.pull-left[

``` r
library(semPlot)
semPaths(m3c_result,what = "mod",
         whatLabels = "std",
         layout = "tree2",
         edge.label.cex = 1,
         edge.label.position = 0.5,
         edge.color = "black", 
         color = "white")
```
]

.pull-right[

![](PSYC5318W04Slides_files/figure-html/unnamed-chunk-20-1.png)&lt;!-- --&gt;

]

---

## Factor scores (what they are)

**Factor scores** are **estimated** latent trait values for each person, computed from the observed indicators **and** the fitted CFA/SEM model.

A model-based “best guess” of a person’s standing on a factor, usually a **weighted composite score** of observed variables

\\[
\hat{\eta} = \mathbf{w}^\top \mathbf{x}
\\]

Where the weights generally depend on loadings and error variances in the model:
- higher loadings → typically more weight  
- more measurement error → typically less weight

---

## Factor scores (why to be cautious)

- They are **model-dependent** and not “true scores”.

- The scores can differ slightly by **scoring method** (regression, Bartlett, etc.)

- If your goal is relationships among constructs, **latent-on-latent** approach is usually recommended over factor scores

---
## Factor Scores in lavaan


``` r
lavPredict(m3c_result, newdata = CFAvars,
           method = "regression")
```


---

## CFA Practical workflow

1. Specify model(s) from theory
2. Inspect data (missingness, distributions, ordinal vs continuous)
3. Choose estimator and scaling
4. Fit model; confirm convergence and admissible solution
5. Evaluate global fit bundle (χ², CFI/TLI, RMSEA, SRMR, etc.)
6. Inspect parameters (loadings, residual variances, factor correlations)
7. Inspect local fit (residuals, MI) cautiously
8. Compare competing models (theory-driven)
9. Finalize and report transparently (including any exploratory steps)

???

The “ethical” part is baked into the workflow: you constrain first, then diagnose, then justify, then validate, then report.

---

## Reporting checklist template


- Sample size; data type (continuous/ordinal); missing handling
- Descriptives and correlation matrix
- Estimator (ML, robust ML, WLSMV) and scaling method
- Model specification (theoretical indication)
- Global fit: χ²(df), CFI, TLI, RMSEA (CI), SRMR
- Key parameters that worth mentioning
- Any respecification: what was changed, when, and why
- Diagrams as visualizations

???

A reader should be able to reconstruct your model and evaluate whether your modifications were principled.

---

### If using lavaan() and the full model specification


``` r
m3c &lt;- '
# 3 latent factors
Fluid  =~ F1 + F2 + F3
Verbal =~ V1 + V2 + V3
Spatial =~ S1 + S2 + S3

# Factor covariances
Fluid ~~ Verbal
Fluid ~~ Spatial
Verbal ~~ Spatial

# Factor variances (unit variance)
Fluid ~~ 1*Fluid
Verbal ~~ 1*Verbal
Spatial ~~ 1*Spatial

# Residual variances
F1 ~~ F1
F2 ~~ F2
F3 ~~ F3
V1 ~~ V1
V2 ~~ V2
V3 ~~ V3
S1 ~~ S1
S2 ~~ S2
S3 ~~ S3

# Intercepts (will still be specified by default)
F1 ~ 1
F2 ~ 1
F3 ~ 1
V1 ~ 1
V2 ~ 1
V3 ~ 1
S1 ~ 1
S2 ~ 1
S3 ~ 1
'
```

---
### More types of model specifications

#### Higher-order model


``` r
m3h &lt;- '
Fluid  =~ F1 + F2 + F3
Verbal =~ V1 + V2 + V3
Spatial =~ S1 + S2 + S3
# A higher-order factor
g =~ Fluid + Verbal + Spatial
'
m3h_result &lt;- cfa(m3h, data = CFAvars,
                  orthogonal = F, std.lv = T)
summary(m3h_result, standardized = T, fit.measures = T)
```

---
### More types of model specifications

#### Bi-factor model


``` r
m3b &lt;- '
Fluid  =~ F1 + F2 + F3
Verbal =~ V1 + V2 + V3
Spatial =~ S1 + S2 + S3
# A general factor and 3 sub-factors
g =~ F1 + F2 + F3 + V1 + V2 + V3 + S1 + S2 + S3
'
m3b_result &lt;- cfa(m3b, data = CFAvars,
                  orthogonal = F, std.lv = T)
summary(m3b_result, standardized = T, fit.measures = T)
```

---

## Readings and Materials

The Kline book

The official lavaan webpage: https://lavaan.ugent.be/

Dr. David Kenny's web tutorials: https://davidakenny.net/cm/causalm.htm

---
class: inverse center middle
# End

    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script src="macros.js"></script>
<script>var slideshow = remark.create({
  "highlightStyle": "github",
  "highlightLines": true,
  "countIncrementalSlides": false,
  "ratio": "16:9"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
